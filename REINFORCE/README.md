# REINFORCE 策略梯度算法




## 实验

1： 完全没学习到。


1to2的消融实验：
想法1：源代码层数较多，隐藏层较小导致不好训练。 =》调整参数。
实验结果：提升很少。 发现实验loss较大，这是因为reward没有进行归一化造成的，log_prob数值正常。

想法2：加入reward归一化。
实验结果：提升很少。 loss和reward平稳，log_prob数值分布范围为-0.4~-0.9这样，几乎不变，说明log_prob不收敛。
怀疑是梯度问题。

想法3：将两处“requires_grad=True”删去，并使用其他方法解决梯度问题。
实验结果：两级分化，要么从一开始就loss归零，要么就是在reward上升到200~300左右后归零，现象与实验2相似。
能训练到500，但最后还是会陷入灾难性遗忘。

实验结论：实验1问题出在“requires_grad=True”上。