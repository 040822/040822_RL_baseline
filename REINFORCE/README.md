# REINFORCE 策略梯度算法




## 实验

1： 完全没学习到。2024.08.28

2:  让gpt4o重新写了版代码，然后将源代码逐行与gpt4o所写的代码对齐，现在源代码在某些情况下能达到500分并在百轮内长期维持100分+的状态，但随后会陷入灾难性遗忘的情况，表现为：reward为10分左右，loss趋近于0，log_prob趋近于0（对应prob趋近于1） 2024.10.20

1to2的消融实验：
想法1：源代码层数较多，隐藏层较小导致不好训练。 =》调整参数。
实验结果：提升很少。 发现实验loss较大，这是因为reward没有进行归一化造成的，log_prob数值正常。

想法2：加入reward归一化。
实验结果：提升很少。 loss和reward平稳，log_prob数值分布范围为-0.4~-0.9这样，几乎不变，说明log_prob不收敛。
怀疑是梯度问题。

想法3：将两处“requires_grad=True”删去，并使用其他方法解决梯度问题。
实验结果：两级分化，要么从一开始就loss归零，要么就是在reward上升到200~300左右后归零，现象与实验2相似。
能训练到500，但最后还是会陷入灾难性遗忘。

实验结论：实验1问题出在“requires_grad=True”上。 2024.10.21

下一步的思路：换其他算法（bushi）。 尝试通过给log_prob加一个下限的方式来避免梯度消失。